# Chapter 4 모델훈련
-   많은 경우 머신러닝의 구현의 상세 사항을 실제로 알아야 할 필요는 없다.
	-   하지만 어떻게 작동하는지 잘 이해하고 있다면 적절한 모델, 올바른 훈련 알고리즘, 작업에 맞는 좋은 하이퍼파라미터를 빠르게 찾을 수 있다.
-   또한 작동 원리를 이해하고 있다면 디버깅이나 에러를 효율적으로 분석하는데 도움이 된다.

## 선형 회귀
- 다음은 1장에서 본 삶의 만족도에 대한 간단한 선형 회귀 모델이다.
    - (삶의 만족도) $= \theta_0 + \theta_1 \times$ (1인당 GDP)
    - 이 모델은 입력 특성인 (1인당 GDP)에 대한 선형 함수이고, $\theta_0$과 $\theta_1$이 모델 파라미터이다.
    - 더 일반적으로 선형 모델은 다음 식에서처럼 입력 특성의 가중치합과 편향(절편)이라는 상수를 더해 예측을 만든다.
     $$\hat y = \theta_0 + \theta_1x_1 + \dots + \theta_nx_n$$
        - $\hat y$는 예측값이다.
        - $n$은 특성의 수이다.
        - $x_i$는 $i$번째 특성값이다.
        - $\theta_i$는 $j$번째 모델 파라미터이다. (편향 $\theta_0$와 특성의 가중치 $\theta_1, \theta_2, \dots, \theta_n$을 포함한다)
		
    - 위의 식은 벡터 형태로 더 간단하게 쓸 수 있다. $$\hat y = h_\theta (x) = \theta \circ x$$
		- $\theta$는 편향 $\theta_0$과 $\theta_1$부터 $\theta_n$까지의 특성 가중치를 담은 모델의 파라미터 벡터이다.
		- $x$는 $x_0$부터 $x_n$까지 담은 샘플의 특성 벡터이다. 이때 $x_0 = 1$ 이다.
		- $\theta \circ x$는 벡터 $\theta$와 $x$의 점곱이다. 이는 $\theta_0x_0 + \theta_1x_1 + \dots + \theta_nx_n$와 같다.
		- $h_\theta$는 모델 파라미터 $\theta$를 사용한 가설(hypothesis) 함수이다.
		
- 머신러닝에서는 벡터를 하나의 열을 가진 2차원 배열인 열 벡터로 나타낸다.
	- $\theta$와 $x$가 열벡터라면 예측은 $\hat y = \theta^Tx$이다.
	- 예측 결과는 같지만 스칼라 값이 아니라 하나의 원소를 가진 행렬이 만들어진다..
	
- 모델을 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것이다.
    - 이를 위해서는 먼저 모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정해야 한다
    - 회귀에 가장 널리 사용되는 성능 측정 지표는 평균 제곱근 오차(RMSE)라 했다.
        - 따라서 선형 회귀 모델을 훈련시키려면 RMSE를 최소화하는 $\theta$를 찾아야 한다.
        - 실제로는 RMSE보다 평균 제곱 오차(MSE)를 최소화하는 것이 같은 결과를 내면서 더 간단하다.
            - 어떤 함수를 최소화하는 것은 그 함수의 제곱근을 최소화하는 것과 같기 때문이다.
			
- 훈련 세트 $X$에 대한 선형 회귀 가설 $h_\theta$는 다음과 같이 계산한다.
    $MSE(X, h_\theta) = \dfrac 1 m \sum ^ m _{i=1} (\theta^Tx^{(i)} - y^{(i)})^2$

### 정규방정식
- 비용 함수를 최소화하는 $\theta$를 찾기 위한 해석적인 방법, 즉 결과를 얻을 수 있는 수학 공식이 있다.
    - 이를 정규방정식이라 한다. 다음과 같이 작성된다.
    - $\hat \theta = (X^TX) ^{-1} X^Ty$
        - $\hat \theta$는 비용 함수를 최소화하는 $\theta$값이다.
        - $y$는 $y^{(1)}$부터 $y^{(m)}$까지 포함하는 타깃 벡터이다.

- 다음과 같이 numpy로 계산을 수행해볼 수 있다.

- 또한, sklearn에 기본 내장된 모듈을 이용해 선형 회귀에 대한 학습을 수행해볼 수 있다.
	- 이때 유사역행렬을 사용할 수 있다.
		- 유사 역행렬은 특잇값 분해(Singular Value Decomposition)라 부르는 표준 행렬 분해 기법을 사용해 계산한다.
		- 정규방정식을 계산하는것보다 이렇게 연산하여 처리할 수 있다.
		- 또한 극단적인 방법도 이를 통해 처리할 수 있다.
			- 실제로 m < n이거나 어떤 특성이 중복되어 행렬 $X^TX$ 의 역행렬이 존재하지 않는다면 (특이 행렬이라면) 정규방정식이 작동하지 않는다.
			- 유사 역행렬은 항상 구할 수 있다.
		
#### 계산 복잡도
- 정규방정식은 $(n + 1) \times (n + 1)$ 크기가 되는 $X^TX$ 의 역행렬을 계산한다. (n은 특성 수)
	- 역행렬을 계산하는 게산복잡도는 일반적으로 $O(n^{2.4})$  에서 $O(n^3)$ 사이이다.
		- 특성 수가 두배 늘어나면 계산 시간이 약 $2^2.4 = 5.3$ 배에서 $2^3 = 8$ 배 증가하게 된다.
	- 사이킷런의 LinearRegression 클래스가 사용하는 SVD 방식은 약 $O(n^2)$이다.
		- 특성의 수가 두배 늘어나면 계산 시간이 대략 4배가 된다.
	- 정규방정식과 SVD 방법 모두 특성수가 많아지면 매우 느려진다.
		- 하지만 훈련 세트의 샘플 수에 대해서는 선형적으로 증가한다.
		- 따라서 메모리 공간이 충분하다면 큰 훈련 세트도 효율적으로 처리할 수 있다.
- 또한 정규방정식이나 다른 알고리즘으로 학습된 선형 회귀 모델은 예측이 매우 빠르다.
	- 예측 계산 복잡도는 샘플 수와 특성 수에 선형적이다.
	- 예측하려는 샘플이 두 배로 늘어나면 (특성이 두배로 늘어나면) 걸리는 시간도 거의 두배 증가한다.
- 정리하면, 정규 방정식은 이러한 $\theta$들을 직접 구하는 방법이라고 볼 수 있다.
	- 특성이 적은 경우 유리할 수 있다.

### 경사 하강법
- 경사 하강법 (Gradient Descent, GD)은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘이다.
	- 기본 아이디어는 비용 함수를 최소화하기 위하여 반복해서 파라미터를 조정하는 것이다.
	- 짙은 안개가 진 산속에서 길을 잃었다고 하자. 이내 발밑 지면의 기울기만 느낄 수 있다.
		- 빠르게 골짜기로 내려가는 좋은 방법은 가장 가파른 길을 따라 아래로 내려가는 것이다.
		- 이것이 경사 하강법의 원리이다.

- 파라미터 벡터 $\theta$ 에 대해 비용 함수의 현재 그레이디언트를 계산한다.
	- 그리고 그라디언트가 감소하는 방향으로 진행한다.
	- 그라디언트가 0이 되면 최솟값에 도달한 것이다.

- 구체적으로 보면 $\theta$를 임의의 값으로 시작해서 (무작위 초기화) 한번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킨다.
- 경사 하강법에서 중요한 파라미터는 스텝의 크기이다.
	- 학습률 하이퍼파라미터로 결정된다.
	- 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸린다.
	- 학습률이 너무 크면 극점을 가로질러 반대편으로 건너뛰게 되어 이전보다 더 높은 곳으로 올라간다.
		- 이는 알고리즘을 더 큰 값으로 발산하게 한다.
- 경사 하강법의 단점 - 모든 비용 함수가 매끈한 곡선이 아니다 
	- 특이한 지형이 있는 경우 최솟값으로 수렴하기 매우 어렵다.
				![[Screen Shot 2022-03-13 at 22.32.25.png]]
		- 다음과 같은 지형이 있는 경우 무작위 초기화에 의해 왼쪽에서 알고리즘이 시작하게 되면 전역 최솟값보다 덜 좋은 지역 최솟값에 수렴한다.
	- 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두점을 선택해 선을 그어도 곡선을 가로지르지 않는 볼록 함수이다.
		- 지역 최솟값이 없고 하나의 전역 최솟값만 존재한다.
			- 이를 통해 경사 하강법이 전역 최솟값에 가깝게 접근할 수 있다는 것을 보장한다. (학습률이 너무 높지 않고 충분히 시간이 주어질 때)
			- 다음 그림과 같이 왼쪽 경사 하강법 알고리즘이 최솟값으로 진행하고 있어 빠르게 도달한다.
				- 오른쪽 그래프는 처음에 전역 최솟값 방향에 거의 직각으로 향하다 평편한 골짜기를 길게 돌아 나가, 최솟값에 도달하겠지만 시가니 오래 걸린다.
![[Screen Shot 2022-03-13 at 22.42.00.png]]
- 경사 하강법을 사용할 때는 반드시 모든 특성이 같은 스케일을 갖도록 만들어야 한다. 그렇지 않으면 수렴에 더 큰 시간이 필요하다.
#### 배치 경사 하강법
- 경사 하강법을 구현하려면 각 모델 파라미터 $\theta_j$ 에 대해 비용 함수의 그레이디언트를 계산해야 한다.
	- $\theta_j$ 가 변경될 때 비용함수가 얼마나 바뀌는지를 계산해야 한다.
	- 이를 편도함수라고 한다.
- 편도함수를 각각 계산하는 대신 다음과 같이 한번에 계산할 수 있다.
	- 그레이디언트 벡터 $\nabla_\theta MSE(\theta)$ 는 비용 함수의 편도함수를 모두 담고있다.
	$$\nabla_\theta MSE(\theta) = \begin{pmatrix} \dfrac \sigma {\sigma \theta_0} MSE (\theta) \\ \dfrac \sigma {\sigma \theta_1} MSE (\theta) \\ \dots \\ \dfrac \sigma {\sigma \theta_n} MSE (\theta) \end{pmatrix} = \dfrac 2 m X^T (X\theta - y)$$
	- 비용 함수의 그레이디언트 벡터는 다음과 같다
		- 이 공식은 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해서 계산한다. 그래서 이 알고리즘을 배치 경사 하강법이라 한다.
		- 매 스텝에서 훈련 데이터 전체를 사용한다.
			- 매우 큰 훈련 세트에서는 아주 느리다.
		- 경사 하강법은 특성 수에 민감하지 않다.
			- 수십만 개의 특성에서 선형 회귀를 훈련시키려면 정규방정식이나 SVD 분해보다 경사 하강법을 사용하는 것이 빠르다.
- 위로 향하는 그레이디언트 벡터가 구해지면 반대 방향인 아래로 가야 한다.
	- $\theta$ 에서 $\nabla_\theta MSE (\theta)$를 빼야한다는 뜻이다. 
	- 여기서 학습률 $\eta$ 가 사용된다.
	- 내려가는 스텝의 크기를 결정하기 위해 그레이디언트 벡터에 $\eta$를 곱한다.
	- 경사 하강법의 스텝
		$$\theta ^{(next step)} = \theta - \eta \nabla_\theta MSE(\theta)$$
	- 적절한 학습률을 찾는 것이 알고리즘의 동작에 중요하게 작동하게 된다.
		- 적절한 학습률을 찾기 위해 그리드 탐색을 사용한다.
			- 하지만 그리드 탐색에서 수렴하는데 너무 오래걸리는 모델을 막기 위해 반복 횟수를 정해야 한다.
		- 반복 횟수는 어떻게 정해야 할까?
			- 너무 작으면 최적점에 도달하기 전에 알고리즘이 멈춘다.
			- 너무 크면 모델 파라미터가 더는 변하지 않는 동안 시간을 낭비하게 된다.
			- 간단한 해결책은 반복 횟수를 아주 크게 지정하고 그레이디언트 벡터가 아주 작아지면, 알고리즘을 중지하는 것이다.
				- 이렇게 되면 벡터의 노름이 어떤 값 $\epsilon$보다 작아지면 경사 하강법이 최솟값에 도달하게 되는 것이다.
			- 수렴율
				- 비용 함수가 볼록 함수이고 기울기가 급격하게 바뀌지 않는 경우, 학습률을 고정한 배치 경사 하강법은 어느정도 시간이 걸려도 최적의 솔루션에 수렴할 수 있다.
				- 비용 함수의 모양에 따라 달라지겠지만 $\epsilon$의 범위 안에서 최적의 솔루션에 도달하기 위해서 $O(1 / \epsilon)$의 반복이 걸릴 수 있다.
					- 더 정확한 최솟값을 엇기 위해 허용 오차를 1/10으로 줄이면 알고리즘의 반복은 10배 늘어날 것이다.
#### 확률적 경사 하강법
- 배치 경사 하강법의 문제
	- 매 스텝에서 전체 훈련 세트를 사용해 그레이디언트를 계산해야 한다.
		- 따라서 훈련 세트가 커지면 매우 느려지게 된다.
- 반대로 확률적 경사 하강법은 매 스탭에서 한 개의 샘플을 무작위로 선택하고, 그 하나의 샘플에 대한 그레이디언트를 계산한다.
	- 매 반복에서 다루어야 할 데이터가 매우 적기 때문에 한 번에 하나의 샘플을 처리하면 알고리즘이 빠르게 작동한다.
	- 매 반복에서 하나의 샘플만 메모리에 있으면 되기 때문에 매우 큰 훈련 세트도 훈련시킬 수 있다.
- 반면, 확률적(즉, 무작위)이므로 이 알고리즘은 배치 경사 하강법보다 훨씬 불안정하다.
	- 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치며 평균적으로 감소한다.
	- 시간이 지나면 최솟값에 매우 근접하지만, 요동이 지속되면 최솟값에 안착하지 못할 것이다.
	- 알고리즘이 멈출때 좋은 파라미터를 구할 수 있지만, 이 파라미터가 최소는 아닐 것이다.
- 비용 함수가 매우 불규칙할 때 알고리즘이 지역 최솟값을 건너뛰도록 도와주므로 확률적 경사 하강법이 배치 경사 하강법보다 전역 최솟값을 찾을 가능성이 높다.
- 무작위성은 알고리즘을 지역 최솟값에서 탈출 시키지만 전역 최솟값에 다다르지 못하게 할 수 있다.
	- 학습률을 점진적으로 감소시켜 이 문제를 해결할 수 있다.
		- 시작할 때 학습률을 크게 하여 빠르게 수렴하고 지역 최솟값에 빠지지 않게 한다.
		- 이후 점차 작게 줄여 알고리즘이 전역 최솟갑셍 도달하도록 한다.
		- 이 과정은 금속공학 분야에서 가열한 금속을 천천히 냉각시키는 담금질 깁법 알고리즘과 유사하다.
		- 매 반복에서 학습률을 결정하는 함수를 학습 스케쥴이라고 부른다. 
			- 학습률이 너무 빨리 줄어들면 지역 최솟값에 갇히거나 최솟값까지 가는 중간에 멈춰버릴 수 있다.
			- 학습률이 너무 천천히 줄어들면 오랫동안 최솟값 주변을 맴돌거나 훈련을 너무 일찍 중지하여 지역 최솟값에 머무를 수 있다.
			- 일반적으로 한 반복에 m번 되풀이 되고, 이때 각 반복을 에포크라고 한다.
			- 샘플을 무작위로 선택하기 때문에 어떤 샘플은 한 에포크에서 여러분 선택될 수 있고 어떤 샘플은 전혀 선택되지 못할 수 있다.
				- 알고즘이 에포크마다 모든 샘플을 사용하게 하려면 훈련 세트를 섞은 후 차ㅐ로 하나씩 선택하고 다음 에포크에서 다시 섞는 식의 방법을 사용할 수 있다.
					- 이렇게  하ㄴ 더 늦게 수렴된다.
#### 미니배치 경사 하강법
- 각 스탭에서 전체 훈련 세트 (배치 경사 하강법)나 하나의 샘플(확률적 경사 하강법)과 유사하게 작동한다.
	- 이때 미니배치라고 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산한다.
- 확률적 경사 하강법에 비해 미니배치 경사 하강법의 주요 장점은 GPU를 사용해서 얻는 성능 향상이다.
	- 미니배치를 어느정도 크게 하면 이 알고리즘은 파라미터 공간에서 SGD보다 덜 불규칙하게 움직인다.
		- 미니배치 경사 하강법이 SGD보다 최솟값에 더 가까이 도달하게 될 것이다.
		- 하지만 지역 최솟값에서 빠져나오기는 더 힘들 수 있다.
![[Screen Shot 2022-03-14 at 14.52.14.png]]
- 확률적 / 배치 / 미니 배치에 대해 다 조사한 경우이다.
	- 모든 방법이 최솟값 근처에 도달했다.
	- 이때 배치 경사 하강법의 경로만 실제 최솟값에 도달했지만, 다른 방법은 근처에서 맴돌고 있다.
### 정리
![[Screen Shot 2022-03-14 at 14.58.04.png]] 
## 다항 회귀
- 가지고 있는 데이터가 단순한 직선보다 복잡한 형태라면 어떨까?
	- 비선형 데이터 학습에 선형 모델을 사용할 수 있다.
		- 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련 시키는 것이다.
		- 이러한 기법을 다항 회귀라고 한다.
		- 